#!/usr/bin/env python3
"""
å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import tensorflow as tf
import numpy as np
import os
import json
from pathlib import Path
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

def load_dataset_info(dataset_path):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    """
    info_path = Path(dataset_path) / "dataset_info.json"
    with open(info_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_data_generators(dataset_path, img_size=(224, 224), batch_size=32):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
    """
    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´ç”¨ï¼‰
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    
    # æ¤œè¨¼ç”¨ï¼ˆæ‹¡å¼µãªã—ï¼‰
    val_datagen = ImageDataGenerator(rescale=1./255)
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ä½œæˆ
    train_generator = train_datagen.flow_from_directory(
        Path(dataset_path) / "train",
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )
    
    val_generator = val_datagen.flow_from_directory(
        Path(dataset_path) / "validation",
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )
    
    return train_generator, val_generator

def create_model(num_classes, img_size=(224, 224)):
    """
    è»¢ç§»å­¦ç¿’ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    """
    # MobileNetV2ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨
    base_model = MobileNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(*img_size, 3)
    )
    
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å‡çµ
    base_model.trainable = False
    
    # æ–°ã—ã„åˆ†é¡å±¤ã‚’è¿½åŠ 
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.3)(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=base_model.input, outputs=predictions)
    
    return model

def train_model(model, train_generator, val_generator, epochs=50):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    """
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7
        ),
        tf.keras.callbacks.ModelCheckpoint(
            'best_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # è¨“ç·´å®Ÿè¡Œ
    history = model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )
    
    return history, model

def plot_training_history(history):
    """
    è¨“ç·´å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç²¾åº¦
    ax1.plot(history.history['accuracy'], label='è¨“ç·´ç²¾åº¦')
    ax1.plot(history.history['val_accuracy'], label='æ¤œè¨¼ç²¾åº¦')
    ax1.set_title('ãƒ¢ãƒ‡ãƒ«ç²¾åº¦')
    ax1.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax1.set_ylabel('ç²¾åº¦')
    ax1.legend()
    
    # æå¤±
    ax2.plot(history.history['loss'], label='è¨“ç·´æå¤±')
    ax2.plot(history.history['val_loss'], label='æ¤œè¨¼æå¤±')
    ax2.set_title('ãƒ¢ãƒ‡ãƒ«æå¤±')
    ax2.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax2.set_ylabel('æå¤±')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    dataset_path = "..\dataset"
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±èª­ã¿è¾¼ã¿
    dataset_info = load_dataset_info(dataset_path)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±: {dataset_info}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ä½œæˆ
    train_generator, val_generator = create_data_generators(dataset_path)
    num_classes = len(train_generator.class_indices)
    
    print(f"ğŸ¯ åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")
    print(f"ğŸ“¸ è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {train_generator.samples}")
    print(f"ğŸ” æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: {val_generator.samples}")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = create_model(num_classes)
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    # è¨“ç·´å®Ÿè¡Œ
    print("ğŸš€ è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...")
    history, trained_model = train_model(model, train_generator, val_generator)
    
    # è¨“ç·´å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    plot_training_history(history)
    
    # æœ€çµ‚è©•ä¾¡
    val_loss, val_accuracy = trained_model.evaluate(val_generator)
    print(f"ğŸ“Š æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {val_accuracy:.4f}")
    print(f"ğŸ“Š æœ€çµ‚æ¤œè¨¼æå¤±: {val_loss:.4f}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    trained_model.save('trained_model.h5')
    print("âœ… è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: trained_model.h5")

if __name__ == "__main__":
    main()
