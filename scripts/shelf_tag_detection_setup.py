#!/usr/bin/env python3
"""
æ£šæœ­ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ  ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
from pathlib import Path
from datetime import datetime

def create_detection_classes():
    """æ¤œå‡ºã‚¯ãƒ©ã‚¹ã®å®šç¾©"""
    return {
        "NAME": {"id": 0, "description": "å•†å“åãƒ»å£²ã‚Šå"},
        "PRICE_BASE": {"id": 1, "description": "æœ¬ä½“ä¾¡æ ¼ï¼ˆç¨æŠœï¼‰"},
        "PRICE_TAX": {"id": 2, "description": "ç¨è¾¼ä¾¡æ ¼"},
        "NOTE": {"id": 3, "description": "è²©ä¿ƒæ–‡ãƒ»æ³¨æ„æ›¸ããƒ»ã‚¹ãƒ­ãƒ¼ã‚¬ãƒ³"},
        "UNIT": {"id": 4, "description": "å˜ä½ãƒ»æ•°é‡è¡¨è¨˜"},
        "SYMBOL": {"id": 5, "description": "è¨˜å·ãƒ»è¡¨è¨˜"}
    }

def create_dataset_structure(base_path="shelf_tag_dataset"):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ä½œæˆ"""
    dataset_path = Path(base_path)
    for subdir in ["train", "validation", "test", "annotations", "models", "dictionaries"]:
        (dataset_path / subdir).mkdir(parents=True, exist_ok=True)
    return dataset_path

def main():
    print("ğŸš€ æ£šæœ­æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    
    dataset_path = create_dataset_structure()
    classes = create_detection_classes()
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    config = {
        "dataset_path": str(dataset_path),
        "classes": classes,
        "created_at": datetime.now().isoformat(),
        "version": "1.0.0"
    }
    
    with open(dataset_path / "config.json", 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†: {dataset_path}")

if __name__ == "__main__":
    main()
