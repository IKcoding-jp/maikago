#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿéš›ã®å•†å“ç”»åƒã‚’åé›†ã—ã¦ã€è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
"""

import os
import json
import shutil
from pathlib import Path
import argparse
from datetime import datetime

def create_dataset_structure(base_path="dataset"):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
    """
    dataset_path = Path(base_path)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    train_path = dataset_path / "train"
    val_path = dataset_path / "validation"
    test_path = dataset_path / "test"
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    for path in [train_path, val_path, test_path]:
        path.mkdir(parents=True, exist_ok=True)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã‚’ä½œæˆã—ã¾ã—ãŸ: {dataset_path}")
    return dataset_path

def create_product_categories():
    """
    å•†å“ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©
    """
    categories = {
        "vegetables": {
            "æ–°ãŸã¾ã­ãå°ç®±": {"price": 298, "description": "æ–°ç‰ã­ãå°ç®±"},
            "ãƒˆãƒãƒˆ": {"price": 198, "description": "ãƒˆãƒãƒˆ"},
            "ã‚­ãƒ£ãƒ™ãƒ„": {"price": 158, "description": "ã‚­ãƒ£ãƒ™ãƒ„"},
            "ã«ã‚“ã˜ã‚“": {"price": 98, "description": "ã«ã‚“ã˜ã‚“"},
            "ã˜ã‚ƒãŒã„ã‚‚": {"price": 128, "description": "ã˜ã‚ƒãŒã„ã‚‚"},
            "ãŸã¾ã­ã": {"price": 88, "description": "ãŸã¾ã­ã"},
            "ãƒ”ãƒ¼ãƒãƒ³": {"price": 78, "description": "ãƒ”ãƒ¼ãƒãƒ³"},
            "ãã‚…ã†ã‚Š": {"price": 68, "description": "ãã‚…ã†ã‚Š"},
            "ãªã™": {"price": 98, "description": "ãªã™"},
            "ç™½èœ": {"price": 198, "description": "ç™½èœ"},
        },
        "mushrooms": {
            "ã—ã„ãŸã‘": {"price": 128, "description": "ã—ã„ãŸã‘"},
            "ãˆã®ããŸã‘": {"price": 98, "description": "ãˆã®ããŸã‘"},
            "ã—ã‚ã˜": {"price": 88, "description": "ã—ã‚ã˜"},
            "ã¾ã„ãŸã‘": {"price": 108, "description": "ã¾ã„ãŸã‘"},
            "ãˆã‚Šã‚“ã": {"price": 158, "description": "ãˆã‚Šã‚“ã"},
            "ã¾ã¤ãŸã‘": {"price": 598, "description": "ã¾ã¤ãŸã‘"},
        },
        "fruits": {
            "ã‚Šã‚“ã”": {"price": 158, "description": "ã‚Šã‚“ã”"},
            "ãƒãƒŠãƒŠ": {"price": 98, "description": "ãƒãƒŠãƒŠ"},
            "ã¿ã‹ã‚“": {"price": 128, "description": "ã¿ã‹ã‚“"},
            "ã¶ã©ã†": {"price": 298, "description": "ã¶ã©ã†"},
        }
    }
    
    return categories

def create_dataset_info(categories, dataset_path):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    """
    info = {
        "created_at": datetime.now().isoformat(),
        "total_categories": len(categories),
        "total_products": sum(len(products) for products in categories.values()),
        "categories": {},
        "dataset_path": str(dataset_path),
        "image_size": [224, 224, 3],
        "format": "RGB"
    }
    
    # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’è¿½åŠ 
    for category_name, products in categories.items():
        info["categories"][category_name] = {
            "count": len(products),
            "products": list(products.keys())
        }
    
    # æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    info_path = dataset_path / "dataset_info.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {info_path}")
    return info

def create_label_mapping(categories):
    """
    ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    """
    label_mapping = {}
    label_id = 0
    
    for category_name, products in categories.items():
        for product_name, product_info in products.items():
            label_key = f"{product_name}_{product_info['price']}"
            label_mapping[label_id] = {
                "name": product_name,
                "price": product_info['price'],
                "category": category_name,
                "description": product_info['description'],
                "label_key": label_key
            }
            label_id += 1
    
    return label_mapping

def create_upload_script(dataset_path, categories):
    """
    ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
    """
    script_content = f'''#!/usr/bin/env python3
"""
ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿéš›ã®å•†å“ç”»åƒã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚
"""

import os
import shutil
from pathlib import Path
import argparse

def upload_images(source_dir, dataset_path, category, product):
    """
    ç”»åƒã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    source_path = Path(source_dir)
    target_path = Path(dataset_path) / "train" / category / product
    
    if not source_path.exists():
        print(f"âŒ ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {{source_path}}")
        return
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    target_path.mkdir(parents=True, exist_ok=True)
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    copied_count = 0
    
    for ext in image_extensions:
        for image_file in source_path.glob(f"*{{ext}}"):
            if image_file.is_file():
                target_file = target_path / f"{{copied_count:03d}}{{ext}}"
                shutil.copy2(image_file, target_file)
                copied_count += 1
                print(f"ğŸ“¸ {{image_file.name}} â†’ {{target_file.name}}")
    
    print(f"âœ… {{copied_count}}å€‹ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {{target_path}}")

def main():
    parser = argparse.ArgumentParser(description="å•†å“ç”»åƒã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    parser.add_argument("source_dir", help="ç”»åƒãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("category", help="å•†å“ã‚«ãƒ†ã‚´ãƒª (vegetables, mushrooms, fruits)")
    parser.add_argument("product", help="å•†å“å")
    parser.add_argument("--dataset-path", default="{dataset_path}", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªã¨å•†å“ã‚’è¡¨ç¤º
    categories = {categories}
    
    if args.category not in categories:
        print(f"âŒ ç„¡åŠ¹ãªã‚«ãƒ†ã‚´ãƒª: {{args.category}}")
        print(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒª: {{list(categories.keys())}}")
        return
    
    if args.product not in categories[args.category]:
        print(f"âŒ ç„¡åŠ¹ãªå•†å“: {{args.product}}")
        print(f"åˆ©ç”¨å¯èƒ½ãªå•†å“: {{list(categories[args.category].keys())}}")
        return
    
    upload_images(args.source_dir, args.dataset_path, args.category, args.product)

if __name__ == "__main__":
    main()
'''
    
    script_path = dataset_path / "upload_images.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    # å®Ÿè¡Œæ¨©é™ã‚’ä»˜ä¸ï¼ˆUnixç³»ã‚·ã‚¹ãƒ†ãƒ ï¼‰
    os.chmod(script_path, 0o755)
    
    print(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ: {script_path}")
    return script_path

def create_training_script(dataset_path, label_mapping):
    """
    è¨“ç·´ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
    """
    script_content = f'''#!/usr/bin/env python3
"""
å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import tensorflow as tf
import numpy as np
import os
import json
from pathlib import Path
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

def load_dataset_info(dataset_path):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    """
    info_path = Path(dataset_path) / "dataset_info.json"
    with open(info_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_data_generators(dataset_path, img_size=(224, 224), batch_size=32):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
    """
    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´ç”¨ï¼‰
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    
    # æ¤œè¨¼ç”¨ï¼ˆæ‹¡å¼µãªã—ï¼‰
    val_datagen = ImageDataGenerator(rescale=1./255)
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ä½œæˆ
    train_generator = train_datagen.flow_from_directory(
        Path(dataset_path) / "train",
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )
    
    val_generator = val_datagen.flow_from_directory(
        Path(dataset_path) / "validation",
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )
    
    return train_generator, val_generator

def create_model(num_classes, img_size=(224, 224)):
    """
    è»¢ç§»å­¦ç¿’ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    """
    # MobileNetV2ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨
    base_model = MobileNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(*img_size, 3)
    )
    
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å‡çµ
    base_model.trainable = False
    
    # æ–°ã—ã„åˆ†é¡å±¤ã‚’è¿½åŠ 
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.3)(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=base_model.input, outputs=predictions)
    
    return model

def train_model(model, train_generator, val_generator, epochs=50):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    """
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7
        ),
        tf.keras.callbacks.ModelCheckpoint(
            'best_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # è¨“ç·´å®Ÿè¡Œ
    history = model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )
    
    return history, model

def plot_training_history(history):
    """
    è¨“ç·´å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç²¾åº¦
    ax1.plot(history.history['accuracy'], label='è¨“ç·´ç²¾åº¦')
    ax1.plot(history.history['val_accuracy'], label='æ¤œè¨¼ç²¾åº¦')
    ax1.set_title('ãƒ¢ãƒ‡ãƒ«ç²¾åº¦')
    ax1.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax1.set_ylabel('ç²¾åº¦')
    ax1.legend()
    
    # æå¤±
    ax2.plot(history.history['loss'], label='è¨“ç·´æå¤±')
    ax2.plot(history.history['val_loss'], label='æ¤œè¨¼æå¤±')
    ax2.set_title('ãƒ¢ãƒ‡ãƒ«æå¤±')
    ax2.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax2.set_ylabel('æå¤±')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    dataset_path = "{dataset_path}"
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±èª­ã¿è¾¼ã¿
    dataset_info = load_dataset_info(dataset_path)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±: {{dataset_info}}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ä½œæˆ
    train_generator, val_generator = create_data_generators(dataset_path)
    num_classes = len(train_generator.class_indices)
    
    print(f"ğŸ¯ åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°: {{num_classes}}")
    print(f"ğŸ“¸ è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {{train_generator.samples}}")
    print(f"ğŸ” æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: {{val_generator.samples}}")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = create_model(num_classes)
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    # è¨“ç·´å®Ÿè¡Œ
    print("ğŸš€ è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...")
    history, trained_model = train_model(model, train_generator, val_generator)
    
    # è¨“ç·´å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    plot_training_history(history)
    
    # æœ€çµ‚è©•ä¾¡
    val_loss, val_accuracy = trained_model.evaluate(val_generator)
    print(f"ğŸ“Š æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {{val_accuracy:.4f}}")
    print(f"ğŸ“Š æœ€çµ‚æ¤œè¨¼æå¤±: {{val_loss:.4f}}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    trained_model.save('trained_model.h5')
    print("âœ… è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: trained_model.h5")

if __name__ == "__main__":
    main()
'''
    
    script_path = dataset_path / "train_model.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    os.chmod(script_path, 0o755)
    
    print(f"âœ… è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ: {script_path}")
    return script_path

def create_readme(dataset_path, categories, label_mapping):
    """
    READMEãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    """
    readme_content = f'''# ã¾ã„ã‚«ã‚´ å•†å“èªè­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦

- **ä½œæˆæ—¥**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ç·å•†å“æ•°**: {len(label_mapping)}
- **ã‚«ãƒ†ã‚´ãƒªæ•°**: {len(categories)}
- **ç”»åƒã‚µã‚¤ã‚º**: 224x224 RGB

## ğŸ—‚ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
{dataset_path}/
â”œâ”€â”€ train/           # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ validation/      # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ test/           # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ dataset_info.json
â”œâ”€â”€ upload_images.py
â”œâ”€â”€ train_model.py
â””â”€â”€ README.md
```

## ğŸ“¦ å•†å“ã‚«ãƒ†ã‚´ãƒª

### é‡èœé¡ (vegetables)
{chr(10).join([f"- {product}: Â¥{info['price']} - {info['description']}" for product, info in categories['vegetables'].items()])}

### ãã®ã“é¡ (mushrooms)
{chr(10).join([f"- {product}: Â¥{info['price']} - {info['description']}" for product, info in categories['mushrooms'].items()])}

### æœç‰©é¡ (fruits)
{chr(10).join([f"- {product}: Â¥{info['price']} - {info['description']}" for product, info in categories['fruits'].items()])}

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. ç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

```bash
python upload_images.py <ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª> <ã‚«ãƒ†ã‚´ãƒª> <å•†å“å>
```

ä¾‹:
```bash
python upload_images.py ./images/onion vegetables æ–°ãŸã¾ã­ãå°ç®±
```

### 2. ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

```bash
python train_model.py
```

### 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã®ç¢ºèª

```bash
python -c "import json; print(json.dumps(json.load(open('dataset_info.json')), indent=2, ensure_ascii=False))"
```

## ğŸ“‹ ç”»åƒåé›†ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### æ’®å½±æ¡ä»¶
- **ç…§æ˜**: è‡ªç„¶å…‰ã¾ãŸã¯æ˜ã‚‹ã„å®¤å†…ç…§æ˜
- **è§’åº¦**: å•†å“ãŒæ­£é¢ã‹ã‚‰è¦‹ãˆã‚‹è§’åº¦
- **èƒŒæ™¯**: ã‚·ãƒ³ãƒ—ãƒ«ãªèƒŒæ™¯ï¼ˆç™½ã¾ãŸã¯è–„ã„è‰²ï¼‰
- **è·é›¢**: å•†å“ãŒç”»é¢ã®70%ä»¥ä¸Šã‚’å ã‚ã‚‹

### å¿…è¦ãªç”»åƒæ•°
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: å„å•†å“ã«ã¤ã50-100æš
- **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: å„å•†å“ã«ã¤ã10-20æš
- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: å„å•†å“ã«ã¤ã5-10æš

### ç”»åƒå½¢å¼
- **å½¢å¼**: JPG, PNG, BMP
- **è§£åƒåº¦**: æœ€ä½224x224ãƒ”ã‚¯ã‚»ãƒ«
- **ãƒ•ã‚¡ã‚¤ãƒ«å**: é€£ç•ªï¼ˆ001.jpg, 002.jpg, ...ï¼‰

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### æ–°ã—ã„å•†å“ã®è¿½åŠ 

1. `categories`è¾æ›¸ã«æ–°ã—ã„å•†å“ã‚’è¿½åŠ 
2. å¯¾å¿œã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
3. ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
4. ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´

### ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®èª¿æ•´

`train_model.py`ã®`ImageDataGenerator`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´:

```python
train_datagen = ImageDataGenerator(
    rotation_range=20,      # å›è»¢è§’åº¦
    width_shift_range=0.2,  # æ°´å¹³ã‚·ãƒ•ãƒˆ
    height_shift_range=0.2, # å‚ç›´ã‚·ãƒ•ãƒˆ
    zoom_range=0.2,         # ã‚ºãƒ¼ãƒ ç¯„å›²
    horizontal_flip=True,   # æ°´å¹³åè»¢
)
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™

- **è¨“ç·´ç²¾åº¦**: 95%ä»¥ä¸Š
- **æ¤œè¨¼ç²¾åº¦**: 90%ä»¥ä¸Š
- **æ¨è«–æ™‚é–“**: 1ç§’ä»¥å†…
- **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º**: 10MBä»¥ä¸‹

## ğŸš¨ æ³¨æ„äº‹é …

- ç”»åƒã®è‘—ä½œæ¨©ã«æ³¨æ„ã—ã¦ãã ã•ã„
- å€‹äººæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- å®šæœŸçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„
- è¨“ç·´å‰ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„
'''
    
    readme_path = dataset_path / "README.md"
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ… READMEãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {readme_path}")

def main():
    parser = argparse.ArgumentParser(description="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
    parser.add_argument("--output", default="dataset", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    
    args = parser.parse_args()
    
    print("ğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ä½œæˆ
    dataset_path = create_dataset_structure(args.output)
    
    # å•†å“ã‚«ãƒ†ã‚´ãƒªå®šç¾©
    categories = create_product_categories()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ä½œæˆ
    dataset_info = create_dataset_info(categories, dataset_path)
    
    # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
    label_mapping = create_label_mapping(categories)
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    upload_script = create_upload_script(dataset_path, categories)
    
    # è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    training_script = create_training_script(dataset_path, label_mapping)
    
    # READMEä½œæˆ
    create_readme(dataset_path, categories, label_mapping)
    
    print("\nğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"\nğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {dataset_path}")
    print(f"ğŸ“ ä½¿ç”¨æ–¹æ³•: {dataset_path}/README.md ã‚’å‚ç…§ã—ã¦ãã ã•ã„")
    print(f"ğŸ“¸ ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: python {upload_script} <ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª> <ã‚«ãƒ†ã‚´ãƒª> <å•†å“å>")
    print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«è¨“ç·´: python {training_script}")

if __name__ == "__main__":
    main()
